{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b479e93",
   "metadata": {},
   "source": [
    "# üß† NLP Foundations Workshop: From Preprocessing to tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c04c3",
   "metadata": {},
   "source": [
    "\n",
    "**Duration**: 90 minutes  \n",
    "**Team Size**: 3 students  \n",
    "**Objective**: Build an NLP pipeline from scratch to implement and test six foundational concepts in Natural Language Processing in preparation for Vector Space Models and Cosine Similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e0c14",
   "metadata": {},
   "source": [
    "## Step 1: Presenting the Six Core NLP Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7f198",
   "metadata": {},
   "source": [
    "### üîπ Term-Document Incidence Matrix\n",
    "\n",
    "The **Term-Document Incidence Matrix** is a binary matrix that shows whether a term $t$ appears in a document $d$.\n",
    "\n",
    "- Rows represent terms in the vocabulary  \n",
    "- Columns represent documents in the corpus  \n",
    "- Each entry $w_{t,d}$ is defined as:\n",
    "\n",
    "$$\n",
    "w_{t,d} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } t \\in d \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This is a **binary representation** ‚Äî it only records the **presence or absence** of a term, not how many times it appears.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Use It?\n",
    "\n",
    "- It‚Äôs the **simplest form** of representing document contents using structured data.\n",
    "- Useful for:\n",
    "  - Boolean search and keyword filters\n",
    "  - Document classification based on keyword sets\n",
    "  - Building foundational **retrieval systems**\n",
    "- Helps in detecting whether **all query terms exist** in a document (e.g., phrase queries or \"AND\" operations)\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Example\n",
    "\n",
    "Suppose we have 3 documents:\n",
    "\n",
    "- **Doc1**: \"machine learning is fun\"  \n",
    "- **Doc2**: \"deep learning is powerful\"  \n",
    "- **Doc3**: \"machine learning and deep models\"\n",
    "\n",
    "The vocabulary extracted from all three is:\n",
    "\n",
    "**Vocabulary** = {machine, learning, is, fun, deep, powerful, and, models}\n",
    "\n",
    "The Term-Document Incidence Matrix would look like:\n",
    "\n",
    "| Term       | Doc1 | Doc2 | Doc3 |\n",
    "|------------|------|------|------|\n",
    "| machine    | 1    | 0    | 1    |\n",
    "| learning   | 1    | 1    | 1    |\n",
    "| is         | 1    | 1    | 0    |\n",
    "| fun        | 1    | 0    | 0    |\n",
    "| deep       | 0    | 1    | 1    |\n",
    "| powerful   | 0    | 1    | 0    |\n",
    "| and        | 0    | 0    | 1    |\n",
    "| models     | 0    | 0    | 1    |\n",
    "\n",
    "For example:\n",
    "- $w_{\\text{machine}, \\text{Doc1}} = 1$ ‚Üí \"machine\" is in Doc1\n",
    "- $w_{\\text{powerful}, \\text{Doc1}} = 0$ ‚Üí \"powerful\" is not in Doc1\n",
    "\n",
    "This matrix is particularly helpful when implementing **Boolean retrieval systems** and **phrase matching**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14ef537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Term-Document Incidence Matrix:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "and",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "deep",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fun",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "machine",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "models",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "powerful",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "37b50348-0250-4a50-9c3f-44374e86b2aa",
       "rows": [
        [
         "Doc1",
         "0",
         "0",
         "1",
         "1",
         "1",
         "1",
         "0",
         "0"
        ],
        [
         "Doc2",
         "0",
         "1",
         "0",
         "1",
         "1",
         "0",
         "0",
         "1"
        ],
        [
         "Doc3",
         "1",
         "1",
         "0",
         "0",
         "1",
         "1",
         "1",
         "0"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>deep</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>models</th>\n",
       "      <th>powerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      and  deep  fun  is  learning  machine  models  powerful\n",
       "Doc1    0     0    1   1         1        1       0         0\n",
       "Doc2    0     1    0   1         1        0       0         1\n",
       "Doc3    1     1    0   0         1        1       1         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìò Example: Term-Document Incidence Matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus from the Markdown example\n",
    "docs = [\n",
    "    \"machine learning is fun\",          # Doc1\n",
    "    \"deep learning is powerful\",        # Doc2\n",
    "    \"machine learning and deep models\"  # Doc3\n",
    "]\n",
    "\n",
    "# Use binary=True to indicate presence/absence (1 or 0)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Create a labeled DataFrame\n",
    "incidence_matrix = pd.DataFrame(X.toarray(),\n",
    "                                index=[\"Doc1\", \"Doc2\", \"Doc3\"],\n",
    "                                columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the incidence matrix\n",
    "print(\"üîé Term-Document Incidence Matrix:\")\n",
    "display(incidence_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bc6a1",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Instructor Talking Point**: This code demonstrates how the presence or absence of a term in a document is encoded as a binary matrix ‚Äî foundational for Boolean retrieval. Explain this with respect to a future AI agent (chatbot) builds context.\n",
    "<br/>\n",
    "<br/>\n",
    "üß† **Student Talking Point**: Add a phrase query (e.g., 'machine learning') and explain your reasoning as to how you would check if both terms occur in a single document using this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e547ea",
   "metadata": {},
   "source": [
    "### üîπ Term Frequency (TF)\n",
    "\n",
    "**Term Frequency (TF)** measures how frequently a term $t$ appears in a document $d$.\n",
    "\n",
    "$$\n",
    "tf_{t,d} = f_{t,d}\n",
    "$$\n",
    "\n",
    "Where $f_{t,d}$ is the raw count of term $t$ in document $d$.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Use It?\n",
    "\n",
    "- TF reflects the importance of a word **within a specific document**.\n",
    "- A higher TF means the term is likely central to the topic of that document.\n",
    "- It's used as the **first step** in vectorizing text for machine learning models like classification, clustering, or information retrieval.\n",
    "\n",
    "TF is most effective when combined with **IDF** (Inverse Document Frequency) to balance against very common terms across the corpus.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Example\n",
    "\n",
    "Let‚Äôs say we have this document:\n",
    "\n",
    "> **Doc1**: `\"machine learning is fun and machine learning is useful\"`\n",
    "\n",
    "Calculate raw term counts:\n",
    "\n",
    "| Term     | Raw TF $(f_{t,d})$ |\n",
    "|----------|--------------------|\n",
    "| machine  | 2                  |\n",
    "| learning | 2                  |\n",
    "| is       | 2                  |\n",
    "| fun      | 1                  |\n",
    "| and      | 1                  |\n",
    "| useful   | 1                  |\n",
    "\n",
    "If normalized (total of 9 words):\n",
    "\n",
    "- $tf(\\text{\"machine\"}, \\text{Doc1}) = \\frac{2}{9} \\approx 0.22$\n",
    "- $tf(\\text{\"learning\"}, \\text{Doc1}) = \\frac{2}{9} \\approx 0.22$\n",
    "\n",
    "This simple frequency can then be used as input into models such as **TF-IDF**, which adjusts these values based on how rare the words are across multiple documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b512919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Raw Term Frequencies:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Raw TF",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cf9dfaa0-611f-4262-91fc-2105e194d092",
       "rows": [
        [
         "0",
         "machine",
         "2"
        ],
        [
         "1",
         "learning",
         "2"
        ],
        [
         "2",
         "is",
         "2"
        ],
        [
         "3",
         "fun",
         "1"
        ],
        [
         "4",
         "and",
         "1"
        ],
        [
         "5",
         "useful",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Raw TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>useful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term  Raw TF\n",
       "0   machine       2\n",
       "1  learning       2\n",
       "2        is       2\n",
       "3       fun       1\n",
       "4       and       1\n",
       "5    useful       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìè Normalized Term Frequencies:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TF (Normalized)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "74b38934-a752-4575-8acb-746c40c6f49e",
       "rows": [
        [
         "0",
         "machine",
         "0.2222222222222222"
        ],
        [
         "1",
         "learning",
         "0.2222222222222222"
        ],
        [
         "2",
         "is",
         "0.2222222222222222"
        ],
        [
         "3",
         "fun",
         "0.1111111111111111"
        ],
        [
         "4",
         "and",
         "0.1111111111111111"
        ],
        [
         "5",
         "useful",
         "0.1111111111111111"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>TF (Normalized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fun</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>useful</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term  TF (Normalized)\n",
       "0   machine         0.222222\n",
       "1  learning         0.222222\n",
       "2        is         0.222222\n",
       "3       fun         0.111111\n",
       "4       and         0.111111\n",
       "5    useful         0.111111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìò Example: Term Frequency (TF)\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Sample document\n",
    "doc1 = \"machine learning is fun and machine learning is useful\"\n",
    "\n",
    "# Tokenize the document (simple lowercase + split)\n",
    "tokens = doc1.lower().split()\n",
    "\n",
    "# Count term frequencies\n",
    "tf_raw = Counter(tokens)\n",
    "\n",
    "# Total number of words\n",
    "total_terms = len(tokens)\n",
    "\n",
    "# Compute normalized TF\n",
    "tf_normalized = {term: count / total_terms for term, count in tf_raw.items()}\n",
    "\n",
    "# Display results\n",
    "print(\"üî¢ Raw Term Frequencies:\")\n",
    "display(pd.DataFrame(tf_raw.items(), columns=[\"Term\", \"Raw TF\"]))\n",
    "\n",
    "print(\"\\nüìè Normalized Term Frequencies:\")\n",
    "display(pd.DataFrame(tf_normalized.items(), columns=[\"Term\", \"TF (Normalized)\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5e206",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Instructor Talking Point**: \"Here we count how often each term appears in a single document and normalize it. This is the simplest way to represent word importance within a document. Explain this with respect to a future AI agent (chatbot) builds  builds context.\n",
    "<br/>\n",
    "<br/>\n",
    "üß† **Student Talking Point**: \"Use this TF output to compare with another document. Which terms are likely to be most important in Doc1 based on their normalized TF? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f347108",
   "metadata": {},
   "source": [
    "### üîπ Log Frequency Weight\n",
    "\n",
    "To reduce the impact of very frequent terms, **log frequency weighting** is applied.\n",
    "\n",
    "$$\n",
    "w_{t,d} =\n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(f_{t,d}) & \\text{if } f_{t,d} > 0 \\\\\n",
    "0 & \\text{if } f_{t,d} = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This transformation reduces the skew caused by terms that appear many times in a document. Instead of allowing their raw frequency to dominate, we scale their contribution **logarithmically**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Use It?\n",
    "\n",
    "- Frequent terms are not always the most **important** terms.\n",
    "- Log scaling ensures that:\n",
    "  - Words with a raw count of 1 are preserved ($1 + \\\\log_{10}(1) = 1$),\n",
    "  - But words with very high counts (e.g., 1000) don‚Äôt dominate the document vector.\n",
    "\n",
    "This helps **normalize the influence** of repetitive terms and improve the **numerical stability** of document representations in models.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Example\n",
    "\n",
    "Let‚Äôs say we have a document with the following raw term counts:\n",
    "\n",
    "| Term     | Raw TF $f_{t,d}$ | Log Frequency Weight $w_{t,d}$ |\n",
    "|----------|------------------|-------------------------------|\n",
    "| machine  | 1                | $1 + \\\\log_{10}(1) = 1$        |\n",
    "| learning | 3                | $1 + \\\\log_{10}(3) \\approx 1.477$ |\n",
    "| data     | 10               | $1 + \\\\log_{10}(10) = 2$       |\n",
    "\n",
    "So even though \"data\" appears 10 times, its log-weighted value is **just 2**, making it more comparable to less frequent but potentially more meaningful terms like \"learning\".\n",
    "\n",
    "This makes log frequency weighting especially useful when preparing inputs for models like **TF-IDF** or **document clustering**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4bcea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Log Frequency Weighting:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Raw TF (f_{t,d})",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Log Weight (w_{t,d})",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8334d1ac-4d16-4784-8412-5806019520e9",
       "rows": [
        [
         "0",
         "machine",
         "2",
         "1.3010299956639813"
        ],
        [
         "1",
         "learning",
         "4",
         "1.6020599913279625"
        ],
        [
         "2",
         "data",
         "7",
         "1.845098040014257"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Raw TF (f_{t,d})</th>\n",
       "      <th>Log Weight (w_{t,d})</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine</td>\n",
       "      <td>2</td>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>4</td>\n",
       "      <td>1.602060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data</td>\n",
       "      <td>7</td>\n",
       "      <td>1.845098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term  Raw TF (f_{t,d})  Log Weight (w_{t,d})\n",
       "0   machine                 2              1.301030\n",
       "1  learning                 4              1.602060\n",
       "2      data                 7              1.845098"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìò Example: Log Frequency Weighting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Sample document with varying term frequencies\n",
    "doc = \"machine learning data data data learning learning learning machine data data data data\"\n",
    "\n",
    "# Tokenize and count raw term frequencies\n",
    "tokens = doc.lower().split()\n",
    "raw_tf = Counter(tokens)\n",
    "\n",
    "# Compute log frequency weights\n",
    "log_weighted_tf = {\n",
    "    term: 1 + np.log10(freq) if freq > 0 else 0\n",
    "    for term, freq in raw_tf.items()\n",
    "}\n",
    "\n",
    "# Build and display the result as a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Term\": raw_tf.keys(),\n",
    "    \"Raw TF (f_{t,d})\": raw_tf.values(),\n",
    "    \"Log Weight (w_{t,d})\": log_weighted_tf.values()\n",
    "})\n",
    "\n",
    "print(\"üìä Log Frequency Weighting:\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278f040",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Instructor Talking Point**: Note how 'data' has a high frequency, but its impact is smoothed by log weighting, making it comparable to 'learning'. Explain this with respect to how a future AI agent (chatbot) builds builds context.\n",
    "<br/>\n",
    "<br/>\n",
    "üß† **Student Talking Point**: Try adjusting the number of times a word appears and observe how the log scale compresses large values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da22b7",
   "metadata": {},
   "source": [
    "### üîπ Document Frequency (DF)\n",
    "\n",
    "**Document Frequency** is the number of documents in which a term $t$ appears:\n",
    "\n",
    "$$\n",
    "df_t = |\\{ d \\in D : t \\in d \\}|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $df_t$ is the document frequency of term $t$\n",
    "- $D$ is the set of all documents in the corpus\n",
    "- $t \\in d$ means the term $t$ appears in document $d$\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Use It?\n",
    "\n",
    "- It helps you understand **how common or rare** a word is across the entire document set.\n",
    "- Words with **high DF** (e.g., ‚Äúthe‚Äù, ‚Äúand‚Äù) occur in many documents and are often **less informative**.\n",
    "- Words with **low DF** are more likely to be **specific and meaningful** for distinguishing between documents.\n",
    "- DF is a key ingredient in calculating **Inverse Document Frequency (IDF)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Example\n",
    "\n",
    "Suppose you have the following three documents:\n",
    "\n",
    "- **Doc1**: \"machine learning is fun\"  \n",
    "- **Doc2**: \"deep learning is powerful\"  \n",
    "- **Doc3**: \"machine learning and deep models\"\n",
    "\n",
    "Now, let‚Äôs compute the Document Frequency:\n",
    "\n",
    "| Term     | Document Frequency ($df_t$) |\n",
    "|----------|-----------------------------|\n",
    "| machine  | 2 (Doc1, Doc3)              |\n",
    "| learning | 3 (Doc1, Doc2, Doc3)        |\n",
    "| deep     | 2 (Doc2, Doc3)              |\n",
    "| models   | 1 (Doc3)                    |\n",
    "\n",
    "The term **\"learning\"** appears in all three documents ‚Üí **high DF**, which means it‚Äôs **less useful for distinguishing** between them.\n",
    "\n",
    "The term **\"models\"** appears in only one document ‚Üí **low DF**, meaning it could be a **useful keyword** for that specific document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "993b6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Document Frequency (DF) Table:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Document Frequency (df_t)",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b2db1b15-1c28-485f-815b-c5995e67b381",
       "rows": [
        [
         "4",
         "learning",
         "3"
        ],
        [
         "1",
         "deep",
         "2"
        ],
        [
         "5",
         "machine",
         "2"
        ],
        [
         "3",
         "is",
         "2"
        ],
        [
         "2",
         "fun",
         "1"
        ],
        [
         "0",
         "and",
         "1"
        ],
        [
         "6",
         "models",
         "1"
        ],
        [
         "7",
         "powerful",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Document Frequency (df_t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>learning</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deep</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>machine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>models</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>powerful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Term  Document Frequency (df_t)\n",
       "4  learning                          3\n",
       "1      deep                          2\n",
       "5   machine                          2\n",
       "3        is                          2\n",
       "2       fun                          1\n",
       "0       and                          1\n",
       "6    models                          1\n",
       "7  powerful                          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìò Example: Document Frequency (DF)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents from Curriculum Learning (4)\n",
    "docs = [\n",
    "    \"machine learning is fun\",          # Doc1\n",
    "    \"deep learning is powerful\",        # Doc2\n",
    "    \"machine learning and deep models\"  # Doc3\n",
    "]\n",
    "\n",
    "# Use CountVectorizer to extract term-document matrix (raw counts)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Get feature names and document-term matrix as array\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Calculate document frequency for each term\n",
    "df_counts = (X_array > 0).sum(axis=0)\n",
    "\n",
    "# Format as a DataFrame\n",
    "df_table = pd.DataFrame({\n",
    "    \"Term\": terms,\n",
    "    \"Document Frequency (df_t)\": df_counts\n",
    "}).sort_values(\"Document Frequency (df_t)\", ascending=False)\n",
    "\n",
    "print(\"üìä Document Frequency (DF) Table:\")\n",
    "display(df_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d74152",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Instructor Talking Point**: Notice how common terms like 'learning' appear in all documents, while more specific terms like 'fun' or 'models' appear in only one.\n",
    "<br/>\n",
    "<br/>\n",
    "üß† **Student Talking Point**: Choose a term and explain how its document frequency could affect downstream TF-IDF weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012a299",
   "metadata": {},
   "source": [
    "### üîπ Inverse Document Frequency (IDF)\n",
    "\n",
    "**Inverse Document Frequency (IDF)** measures how rare or informative a term is across the entire corpus:\n",
    "\n",
    "$$\n",
    "idf_t = \\log_{10} \\left( \\frac{N}{df_t} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the total number of documents in the corpus  \n",
    "- $df_t$ is the number of documents that contain the term $t$\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Use It?\n",
    "\n",
    "- IDF is used to **downweight common terms** and **upweight rare ones**.\n",
    "- Words like ‚Äúthe‚Äù, ‚Äúand‚Äù, or ‚Äúdata‚Äù appear frequently and are less helpful in distinguishing documents.\n",
    "- Terms that appear in **fewer documents** are often **more informative** and **discriminative**.\n",
    "- IDF is a core component of **TF-IDF**, a widely used technique in search engines, document classification, and clustering.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Example\n",
    "\n",
    "Let‚Äôs say we have **5 documents** total, and the following document frequencies:\n",
    "\n",
    "| Term     | $df_t$ | $idf_t = \\log_{10}(N / df_t)$ |\n",
    "|----------|--------|-------------------------------|\n",
    "| machine  | 3      | $\\log_{10}(5 / 3) \\approx 0.22$ |\n",
    "| entropy  | 1      | $\\log_{10}(5 / 1) = 0.70$       |\n",
    "| the      | 5      | $\\log_{10}(5 / 5) = 0.00$       |\n",
    "\n",
    "- The term **\"entropy\"** appears in only one document, so its IDF is **high** ‚Üí it‚Äôs a **rare and informative term**.\n",
    "- The term **\"the\"** ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4553c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Inverse Document Frequency (IDF) Table:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Term",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Document Frequency (df_t)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "IDF (log10(N / df_t))",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1a3103db-93e8-4841-8170-ab8e3369a3df",
       "rows": [
        [
         "0",
         "advanced",
         "1",
         "0.6989700043360189"
        ],
        [
         "1",
         "ai",
         "1",
         "0.6989700043360189"
        ],
        [
         "2",
         "and",
         "1",
         "0.6989700043360189"
        ],
        [
         "3",
         "are",
         "1",
         "0.6989700043360189"
        ],
        [
         "4",
         "deep",
         "1",
         "0.6989700043360189"
        ],
        [
         "5",
         "entropy",
         "1",
         "0.6989700043360189"
        ],
        [
         "6",
         "evolving",
         "1",
         "0.6989700043360189"
        ],
        [
         "11",
         "of",
         "1",
         "0.6989700043360189"
        ],
        [
         "14",
         "science",
         "1",
         "0.6989700043360189"
        ],
        [
         "10",
         "measures",
         "1",
         "0.6989700043360189"
        ],
        [
         "13",
         "randomness",
         "1",
         "0.6989700043360189"
        ],
        [
         "12",
         "powerful",
         "1",
         "0.6989700043360189"
        ],
        [
         "15",
         "the",
         "1",
         "0.6989700043360189"
        ],
        [
         "7",
         "is",
         "2",
         "0.3979400086720376"
        ],
        [
         "9",
         "machine",
         "3",
         "0.2218487496163564"
        ],
        [
         "8",
         "learning",
         "4",
         "0.09691001300805642"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 16
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Document Frequency (df_t)</th>\n",
       "      <th>IDF (log10(N / df_t))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advanced</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ai</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>evolving</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>science</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>measures</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>randomness</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>powerful</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "      <td>0.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>machine</td>\n",
       "      <td>3</td>\n",
       "      <td>0.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>learning</td>\n",
       "      <td>4</td>\n",
       "      <td>0.096910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Term  Document Frequency (df_t)  IDF (log10(N / df_t))\n",
       "0     advanced                          1               0.698970\n",
       "1           ai                          1               0.698970\n",
       "2          and                          1               0.698970\n",
       "3          are                          1               0.698970\n",
       "4         deep                          1               0.698970\n",
       "5      entropy                          1               0.698970\n",
       "6     evolving                          1               0.698970\n",
       "11          of                          1               0.698970\n",
       "14     science                          1               0.698970\n",
       "10    measures                          1               0.698970\n",
       "13  randomness                          1               0.698970\n",
       "12    powerful                          1               0.698970\n",
       "15         the                          1               0.698970\n",
       "7           is                          2               0.397940\n",
       "9      machine                          3               0.221849\n",
       "8     learning                          4               0.096910"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìò Example: Inverse Document Frequency (IDF)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents (5 total)\n",
    "docs = [\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is advanced\",\n",
    "    \"entropy measures randomness\",\n",
    "    \"machine learning and AI are evolving\",\n",
    "    \"the science of machine learning\"\n",
    "]\n",
    "\n",
    "# Total number of documents\n",
    "N = len(docs)\n",
    "\n",
    "# Use CountVectorizer to get document-term matrix\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Compute document frequency for each term\n",
    "df_counts = (X_array > 0).sum(axis=0)\n",
    "\n",
    "# Compute IDF using log base 10\n",
    "idf_values = np.log10(N / df_counts)\n",
    "\n",
    "# Build a DataFrame for display\n",
    "idf_table = pd.DataFrame({\n",
    "    \"Term\": terms,\n",
    "    \"Document Frequency (df_t)\": df_counts,\n",
    "    \"IDF (log10(N / df_t))\": idf_values\n",
    "}).sort_values(\"IDF (log10(N / df_t))\", ascending=False)\n",
    "\n",
    "print(\"üìä Inverse Document Frequency (IDF) Table:\")\n",
    "display(idf_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fdf35",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Instructor Talking Point**: IDF adjusts for the fact that some words are common across all documents ‚Äî this is critical in improving document relevance in search systems.\n",
    "<br/>\n",
    "<br/>\n",
    "üß† **Student Talking Point**: Choose a low-IDF and high-IDF term from this output and explain why they behave differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe918e",
   "metadata": {},
   "source": [
    "### üîπ TF-IDF Weighting\n",
    "\n",
    "**TF-IDF (Term Frequency‚ÄìInverse Document Frequency)** scores each term $t$ in document $d$ based on how frequent and how rare it is:\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\left(1 + \\log_{10}(f_{t,d})\\right) \\times \\log_{10} \\left( \\frac{N}{df_t} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f_{t,d}$ is the raw count of term $t$ in document $d$\n",
    "- $df_t$ is the number of documents that contain term $t$\n",
    "- $N$ is the total number of documents in the corpus\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Use It?\n",
    "\n",
    "- TF-IDF balances **term importance within a document** (TF) against **term commonality across all documents** (IDF).\n",
    "- It **boosts rare, relevant words** while **suppressing frequent, generic words**.\n",
    "- TF-IDF is foundational in:\n",
    "  - Information Retrieval (search engines)\n",
    "  - Document similarity\n",
    "  - Feature engineering for classification or clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### üìò Example\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "- $f_{\\text{machine}, \\text{Doc1}} = 3$\n",
    "- $df_{\\text{machine}} = 2$\n",
    "- $N = 5$ total documents\n",
    "\n",
    "Then:\n",
    "\n",
    "- TF part: $1 + \\log_{10}(3) \\approx 1 + 0.477 = 1.477$\n",
    "- IDF part: $\\log_{10}(5 / 2) \\approx 0.398$\n",
    "- TF-IDF weight:\n",
    "\n",
    "$$\n",
    "w_{\\text{machine}, \\text{Doc1}} = 1.477 \\times 0.398 \\approx 0.588\n",
    "$$\n",
    "\n",
    "This means \"machine\" is **important within Doc1**, but since it's found in other documents too, the overall weight is **moderated**.\n",
    "\n",
    "TF-IDF creates a **sparse, weighted vector representation** of documents, ready for:\n",
    "- Cosine similarity\n",
    "- Clustering\n",
    "- Search ranking\n",
    "- Input into classical machine learning models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f33e9308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TF-IDF Weighted Matrix (Manual Computation):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_20912\\4078578607.py:31: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf_log = 1 + np.where(X_array > 0, np.log10(X_array), 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "advanced",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ai",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "and",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "are",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "deep",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "entropy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "evolving",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "is",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "learning",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "machine",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "measures",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "of",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "powerful",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "randomness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "science",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "the",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "657e00e1-ccbb-421f-9209-62997b348218",
       "rows": [
        [
         "Doc1",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ],
        [
         "Doc2",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ],
        [
         "Doc3",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ],
        [
         "Doc4",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ],
        [
         "Doc5",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.398",
         "0.097",
         "0.222",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699",
         "0.699"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advanced</th>\n",
       "      <th>ai</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>deep</th>\n",
       "      <th>entropy</th>\n",
       "      <th>evolving</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>measures</th>\n",
       "      <th>of</th>\n",
       "      <th>powerful</th>\n",
       "      <th>randomness</th>\n",
       "      <th>science</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      advanced     ai    and    are   deep  entropy  evolving     is  \\\n",
       "Doc1     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "Doc2     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "Doc3     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "Doc4     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "Doc5     0.699  0.699  0.699  0.699  0.699    0.699     0.699  0.398   \n",
       "\n",
       "      learning  machine  measures     of  powerful  randomness  science    the  \n",
       "Doc1     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  \n",
       "Doc2     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  \n",
       "Doc3     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  \n",
       "Doc4     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  \n",
       "Doc5     0.097    0.222     0.699  0.699     0.699       0.699    0.699  0.699  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìò Example: TF-IDF Weighting (Manual Computation)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus of 5 documents\n",
    "docs = [\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is advanced\",\n",
    "    \"entropy measures randomness\",\n",
    "    \"machine learning and AI are evolving\",\n",
    "    \"the science of machine learning\"\n",
    "]\n",
    "\n",
    "# Total number of documents\n",
    "N = len(docs)\n",
    "\n",
    "# Vectorize (raw term frequencies)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Compute Document Frequencies\n",
    "df = (X_array > 0).sum(axis=0)\n",
    "idf = np.log10(N / df)\n",
    "\n",
    "# Manual TF-IDF: apply (1 + log10(tf)) * idf\n",
    "tf_log = 1 + np.where(X_array > 0, np.log10(X_array), 0)\n",
    "tfidf = tf_log * idf\n",
    "\n",
    "# Create a DataFrame for visual inspection\n",
    "tfidf_df = pd.DataFrame(tfidf, columns=terms, index=[f\"Doc{i+1}\" for i in range(N)])\n",
    "\n",
    "print(\"üìä TF-IDF Weighted Matrix (Manual Computation):\")\n",
    "display(tfidf_df.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131478c9",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Instructor Talking Point**: We combined TF and IDF manually ‚Äî useful for seeing how each part of the formula shapes the final result.\n",
    "<br/>\n",
    "<br/>\n",
    "üó£Ô∏è **Instructor Talking Point**: Document Frequency (DF) counts how many documents contain a specific term, showing how common it is across the corpus.\n",
    "Inverse Document Frequency (IDF) does the opposite‚Äîit measures how rare or informative a term is by applying a logarithmic scale to the inverse of DF.\n",
    "So, DF increases with term frequency across documents, while IDF decreases, giving higher weight to rare terms.\n",
    "Together, they balance relevance: DF tells us \"how many use this term,\" while IDF tells us \"how useful is this term for distinguishing documents.\"\n",
    "IDF is critical for reducing noise from overly common words.\n",
    "<br/>\n",
    "<br/>\n",
    "üß† **Student Talking Point**: \"Pick one row (a document) and explain which term seems most important and why, based on the TF-IDF weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2a938",
   "metadata": {},
   "source": [
    "## Step 2: Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc648f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28¬†documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to data\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to data\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\parth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import reuters\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import nltk\n",
    "DATA_DIR = os.path.join(os.path.dirname('data'), 'data', 'nltk_data')\n",
    "\n",
    "# 2. Make sure it exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# 3. Tell NLTK to look in there\n",
    "nltk.data.path.append(DATA_DIR)\n",
    "\n",
    "# 4. Download the Gutenberg corpus into that folder\n",
    "nltk.download('gutenberg', download_dir=DATA_DIR)\n",
    "nltk.download('reuters', download_dir=DATA_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# List of file IDs (there are 18 built-in Gutenberg books)\n",
    "nltk.download('gutenberg')   \n",
    "file_ids = gutenberg.fileids()\n",
    "documents = [gutenberg.raw(file_id) for file_id in file_ids]\n",
    "\n",
    "# If you want 20+ documents, you can duplicate or supplement from other corpora\n",
    "\n",
    "\n",
    "\n",
    "# Add 10 more from Reuters\n",
    "reuters_ids = reuters.fileids()[:10]\n",
    "documents.extend([reuters.raw(doc_id) for doc_id in reuters_ids])\n",
    "\n",
    "print(f\"Loaded {len(documents)}¬†documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8034f9",
   "metadata": {},
   "source": [
    "## Step 3: Implement a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3e744d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: austen-emma.txt ‚Äî 161983 tokens\n",
      "Tokenized: austen-persuasion.txt ‚Äî 84167 tokens\n",
      "Tokenized: austen-sense.txt ‚Äî 120787 tokens\n",
      "Tokenized: bible-kjv.txt ‚Äî 854046 tokens\n",
      "Tokenized: blake-poems.txt ‚Äî 6936 tokens\n",
      "Tokenized: bryant-stories.txt ‚Äî 46703 tokens\n",
      "Tokenized: burgess-busterbrown.txt ‚Äî 16363 tokens\n",
      "Tokenized: carroll-alice.txt ‚Äî 27336 tokens\n",
      "Tokenized: cats.txt ‚Äî 36329 tokens\n",
      "Tokenized: chesterton-ball.txt ‚Äî 82867 tokens\n",
      "Tokenized: chesterton-brown.txt ‚Äî 73288 tokens\n",
      "Tokenized: chesterton-thursday.txt ‚Äî 58729 tokens\n",
      "Tokenized: edgeworth-parents.txt ‚Äî 170796 tokens\n",
      "Tokenized: melville-moby_dick.txt ‚Äî 218621 tokens\n",
      "Tokenized: milton-paradise.txt ‚Äî 80497 tokens\n",
      "Tokenized: shakespeare-caesar.txt ‚Äî 20873 tokens\n",
      "Tokenized: shakespeare-hamlet.txt ‚Äî 30271 tokens\n",
      "Tokenized: shakespeare-macbeth.txt ‚Äî 18351 tokens\n",
      "Tokenized: whitman-leaves.txt ‚Äî 126605 tokens\n",
      "\n",
      "Preview from first document:\n",
      "['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Load and tokenize all .txt files from the 'data/' folder\n",
    "def load_and_tokenize_all(folder_path):\n",
    "    tokenized_documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors= \"ignore\") as file:\n",
    "                text = file.read()\n",
    "                tokens = tokenize(text)\n",
    "                tokenized_documents.append(tokens)\n",
    "                print(f\"Tokenized: {filename} ‚Äî {len(tokens)} tokens\")\n",
    "    return tokenized_documents\n",
    "\n",
    "# Run the function on your data folder\n",
    "folder_path = 'data/nltk_data/corpora'\n",
    "all_tokenized_docs = load_and_tokenize_all(folder_path)\n",
    "\n",
    "# Preview the first 30 tokens from the first document\n",
    "print(\"\\nPreview from first document:\")\n",
    "print(all_tokenized_docs[0][:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd760f4",
   "metadata": {},
   "source": [
    "## Step 4: Text Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d53f7134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: austen-emma.txt ‚Äî 73532 tokens (stopwords removed & stemmed)\n",
      "Processed: austen-persuasion.txt ‚Äî 38383 tokens (stopwords removed & stemmed)\n",
      "Processed: austen-sense.txt ‚Äî 54040 tokens (stopwords removed & stemmed)\n",
      "Processed: bible-kjv.txt ‚Äî 437149 tokens (stopwords removed & stemmed)\n",
      "Processed: blake-poems.txt ‚Äî 3807 tokens (stopwords removed & stemmed)\n",
      "Processed: bryant-stories.txt ‚Äî 21810 tokens (stopwords removed & stemmed)\n",
      "Processed: burgess-busterbrown.txt ‚Äî 7618 tokens (stopwords removed & stemmed)\n",
      "Processed: carroll-alice.txt ‚Äî 12243 tokens (stopwords removed & stemmed)\n",
      "Processed: cats.txt ‚Äî 36329 tokens (stopwords removed & stemmed)\n",
      "Processed: chesterton-ball.txt ‚Äî 39900 tokens (stopwords removed & stemmed)\n",
      "Processed: chesterton-brown.txt ‚Äî 35350 tokens (stopwords removed & stemmed)\n",
      "Processed: chesterton-thursday.txt ‚Äî 28333 tokens (stopwords removed & stemmed)\n",
      "Processed: edgeworth-parents.txt ‚Äî 78207 tokens (stopwords removed & stemmed)\n",
      "Processed: melville-moby_dick.txt ‚Äî 110719 tokens (stopwords removed & stemmed)\n",
      "Processed: milton-paradise.txt ‚Äî 45572 tokens (stopwords removed & stemmed)\n",
      "Processed: shakespeare-caesar.txt ‚Äî 11127 tokens (stopwords removed & stemmed)\n",
      "Processed: shakespeare-hamlet.txt ‚Äî 15903 tokens (stopwords removed & stemmed)\n",
      "Processed: shakespeare-macbeth.txt ‚Äî 10157 tokens (stopwords removed & stemmed)\n",
      "Processed: whitman-leaves.txt ‚Äî 65409 tokens (stopwords removed & stemmed)\n",
      "\n",
      "Preview from first document:\n",
      "['emma', 'jane', 'austen', '1816', 'volum', 'chapter', 'emma', 'woodhous', 'handsom', 'clever', 'rich', 'comfort', 'home', 'happi', 'disposit', 'seem', 'unit', 'best', 'bless', 'exist', 'live', 'nearli', 'twenti', 'one', 'year', 'world', 'littl', 'distress', 'vex', 'youngest']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "\n",
    "def normalize(tokens):\n",
    "    normalized = []\n",
    "    for t in tokens:\n",
    "        if t in STOPWORDS:\n",
    "            continue\n",
    "        stemmed = STEMMER.stem(t)\n",
    "        normalized.append(stemmed)\n",
    "    return normalized\n",
    "\n",
    "# Load, tokenize, normalize all .txt files\n",
    "def load_and_process_all(folder_path):\n",
    "    all_docs = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for fname in files:\n",
    "            if not fname.endswith('.txt'):\n",
    "                continue\n",
    "            path = os.path.join(root, fname)\n",
    "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            tokens = tokenize(text)\n",
    "            tokens = normalize(tokens)\n",
    "            all_docs.append(tokens)\n",
    "            print(f\"Processed: {fname} ‚Äî {len(tokens)} tokens (stopwords removed & stemmed)\")\n",
    "    return all_docs\n",
    "\n",
    "# Run it\n",
    "folder_path = 'data/nltk_data/corpora'\n",
    "all_tokenized_docs = load_and_process_all(folder_path)\n",
    "\n",
    "# Preview the first 30 normalized tokens from the first document\n",
    "print(\"\\nPreview from first document:\")\n",
    "print(all_tokenized_docs[0][:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9bc1c",
   "metadata": {},
   "source": [
    "## Step 5: Build and Test the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23ce7b",
   "metadata": {},
   "source": [
    "\n",
    "Using the six concepts and the preprocessing pipeline above, implement a full pipeline that:\n",
    "- Preprocesses text\n",
    "- Applies vectorization\n",
    "- Computes all six concept metrics\n",
    "- Tests with one phrase query per concept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a8c90",
   "metadata": {},
   "source": [
    "## Term Document Index Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20042114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00  000  00021053  00081429  00482129  01  02  10  100  1000  ...  \\\n",
      "0    0    1         0         0         0   0   0   1    0     0  ...   \n",
      "1    0    0         0         0         0   0   0   1    0     0  ...   \n",
      "2    0    0         0         0         0   0   0   1    0     0  ...   \n",
      "3    0    0         0         0         0   0   0   1    1     0  ...   \n",
      "4    0    0         0         0         0   0   0   0    0     0  ...   \n",
      "5    0    0         0         0         0   0   0   0    0     0  ...   \n",
      "6    0    0         0         0         0   0   0   0    0     0  ...   \n",
      "7    0    0         0         0         0   0   0   0    0     0  ...   \n",
      "8    0    0         0         0         0   0   0   1    1     1  ...   \n",
      "9    1    1         0         0         0   1   1   1    1     1  ...   \n",
      "10   0    0         0         0         0   0   0   0    0     0  ...   \n",
      "11   0    0         0         0         0   0   0   0    0     0  ...   \n",
      "12   0    0         0         0         0   0   0   0    0     0  ...   \n",
      "13   0    1         0         0         0   0   0   1    1     0  ...   \n",
      "14   0    0         1         1         1   0   0   0    0     0  ...   \n",
      "15   0    0         0         0         0   0   0   0    0     0  ...   \n",
      "16   0    0         0         0         0   0   0   0    0     0  ...   \n",
      "17   0    0         0         0         0   0   0   0    0     0  ...   \n",
      "18   0    0         0         0         0   0   0   1    0     0  ...   \n",
      "19   0    0         0         0         0   0   0   0    0     0  ...   \n",
      "\n",
      "    zorobabel  zso  zuar  zumpt  zuph  zur  zuriel  zurishaddai  zuyder  zuzim  \n",
      "0           0    0     0      0     0    0       0            0       0      0  \n",
      "1           0    0     0      0     0    0       0            0       0      0  \n",
      "2           0    0     0      0     0    0       0            0       0      0  \n",
      "3           1    0     1      0     1    1       1            1       0      1  \n",
      "4           0    0     0      0     0    0       0            0       0      0  \n",
      "5           0    0     0      0     0    0       0            0       0      0  \n",
      "6           0    0     0      0     0    0       0            0       0      0  \n",
      "7           0    0     0      0     0    0       0            0       0      0  \n",
      "8           0    0     0      0     0    0       0            0       0      0  \n",
      "9           0    0     0      0     0    0       0            0       0      0  \n",
      "10          0    0     0      0     0    0       0            0       0      0  \n",
      "11          0    1     0      1     0    0       0            0       0      0  \n",
      "12          0    0     0      0     0    0       0            0       0      0  \n",
      "13          0    0     0      0     0    0       0            0       0      0  \n",
      "14          0    0     0      0     0    0       0            0       0      0  \n",
      "15          0    0     0      0     0    0       0            0       0      0  \n",
      "16          0    0     0      0     0    0       0            0       0      0  \n",
      "17          0    0     0      0     0    0       0            0       0      0  \n",
      "18          0    0     0      0     0    0       0            0       1      0  \n",
      "19          0    0     0      0     0    0       0            0       0      0  \n",
      "\n",
      "[20 rows x 36705 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "manual_doc = load_and_tokenize_all('data/')\n",
    "combined_docs = all_tokenized_docs + [manual_doc]\n",
    "combined_texts = [' '.join(doc) for doc in combined_docs]\n",
    "\n",
    "vectorizer_binary = CountVectorizer(binary=True)\n",
    "X_incidence = vectorizer_binary.fit_transform(combined_texts)\n",
    "incidence_df = pd.DataFrame(X_incidence.toarray(), columns=vectorizer_binary.get_feature_names_out())\n",
    "\n",
    "print(incidence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c72ca07",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75709d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00  000  00021053  00081429  00482129  01  02    10  100  1000  ...  \\\n",
      "0    0    2         0         0         0   0   0     2    0     0  ...   \n",
      "1    0    0         0         0         0   0   0     1    0     0  ...   \n",
      "2    0    0         0         0         0   0   0     1    0     0  ...   \n",
      "3    0    0         0         0         0   0   0  2117    6     0  ...   \n",
      "4    0    0         0         0         0   0   0     0    0     0  ...   \n",
      "5    0    0         0         0         0   0   0     0    0     0  ...   \n",
      "6    0    0         0         0         0   0   0     0    0     0  ...   \n",
      "7    0    0         0         0         0   0   0     0    0     0  ...   \n",
      "8    0    0         0         0         0   0   0     1    1     1  ...   \n",
      "9    1    1         0         0         0   1   2     1    2     1  ...   \n",
      "10   0    0         0         0         0   0   0     0    0     0  ...   \n",
      "11   0    0         0         0         0   0   0     0    0     0  ...   \n",
      "12   0    0         0         0         0   0   0     0    0     0  ...   \n",
      "13   0   20         0         0         0   0   0     4    1     0  ...   \n",
      "14   0    0         1         1         1   0   0     0    0     0  ...   \n",
      "15   0    0         0         0         0   0   0     0    0     0  ...   \n",
      "16   0    0         0         0         0   0   0     0    0     0  ...   \n",
      "17   0    0         0         0         0   0   0     0    0     0  ...   \n",
      "18   0    0         0         0         0   0   0     7    0     0  ...   \n",
      "19   0    0         0         0         0   0   0     0    0     0  ...   \n",
      "\n",
      "    zorobabel  zso  zuar  zumpt  zuph  zur  zuriel  zurishaddai  zuyder  zuzim  \n",
      "0           0    0     0      0     0    0       0            0       0      0  \n",
      "1           0    0     0      0     0    0       0            0       0      0  \n",
      "2           0    0     0      0     0    0       0            0       0      0  \n",
      "3           3    0     5      0     3    5       1            5       0      1  \n",
      "4           0    0     0      0     0    0       0            0       0      0  \n",
      "5           0    0     0      0     0    0       0            0       0      0  \n",
      "6           0    0     0      0     0    0       0            0       0      0  \n",
      "7           0    0     0      0     0    0       0            0       0      0  \n",
      "8           0    0     0      0     0    0       0            0       0      0  \n",
      "9           0    0     0      0     0    0       0            0       0      0  \n",
      "10          0    0     0      0     0    0       0            0       0      0  \n",
      "11          0    2     0      1     0    0       0            0       0      0  \n",
      "12          0    0     0      0     0    0       0            0       0      0  \n",
      "13          0    0     0      0     0    0       0            0       0      0  \n",
      "14          0    0     0      0     0    0       0            0       0      0  \n",
      "15          0    0     0      0     0    0       0            0       0      0  \n",
      "16          0    0     0      0     0    0       0            0       0      0  \n",
      "17          0    0     0      0     0    0       0            0       0      0  \n",
      "18          0    0     0      0     0    0       0            0       1      0  \n",
      "19          0    0     0      0     0    0       0            0       0      0  \n",
      "\n",
      "[20 rows x 36705 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_tf = CountVectorizer(binary=False)\n",
    "X_tf = vectorizer_tf.fit_transform(combined_texts)\n",
    "tf_df = pd.DataFrame(X_tf.toarray(), columns=vectorizer_tf.get_feature_names_out())\n",
    "\n",
    "print(tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d63ea",
   "metadata": {},
   "source": [
    "## Log Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d225f816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     00      000  00021053  00081429  00482129   01       02        10  \\\n",
      "0   1.0  1.30103       1.0       1.0       1.0  1.0  1.00000  1.301030   \n",
      "1   1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "2   1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "3   1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  4.325721   \n",
      "4   1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "5   1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "6   1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "7   1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "8   1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "9   1.0  1.00000       1.0       1.0       1.0  1.0  1.30103  1.000000   \n",
      "10  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "11  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "12  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "13  1.0  2.30103       1.0       1.0       1.0  1.0  1.00000  1.602060   \n",
      "14  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "15  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "16  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "17  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "18  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.845098   \n",
      "19  1.0  1.00000       1.0       1.0       1.0  1.0  1.00000  1.000000   \n",
      "\n",
      "         100  1000  ...  zorobabel      zso     zuar  zumpt      zuph  \\\n",
      "0   1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "1   1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "2   1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "3   1.778151   1.0  ...   1.477121  1.00000  1.69897    1.0  1.477121   \n",
      "4   1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "5   1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "6   1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "7   1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "8   1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "9   1.301030   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "10  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "11  1.000000   1.0  ...   1.000000  1.30103  1.00000    1.0  1.000000   \n",
      "12  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "13  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "14  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "15  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "16  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "17  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "18  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "19  1.000000   1.0  ...   1.000000  1.00000  1.00000    1.0  1.000000   \n",
      "\n",
      "        zur  zuriel  zurishaddai  zuyder  zuzim  \n",
      "0   1.00000     1.0      1.00000     1.0    1.0  \n",
      "1   1.00000     1.0      1.00000     1.0    1.0  \n",
      "2   1.00000     1.0      1.00000     1.0    1.0  \n",
      "3   1.69897     1.0      1.69897     1.0    1.0  \n",
      "4   1.00000     1.0      1.00000     1.0    1.0  \n",
      "5   1.00000     1.0      1.00000     1.0    1.0  \n",
      "6   1.00000     1.0      1.00000     1.0    1.0  \n",
      "7   1.00000     1.0      1.00000     1.0    1.0  \n",
      "8   1.00000     1.0      1.00000     1.0    1.0  \n",
      "9   1.00000     1.0      1.00000     1.0    1.0  \n",
      "10  1.00000     1.0      1.00000     1.0    1.0  \n",
      "11  1.00000     1.0      1.00000     1.0    1.0  \n",
      "12  1.00000     1.0      1.00000     1.0    1.0  \n",
      "13  1.00000     1.0      1.00000     1.0    1.0  \n",
      "14  1.00000     1.0      1.00000     1.0    1.0  \n",
      "15  1.00000     1.0      1.00000     1.0    1.0  \n",
      "16  1.00000     1.0      1.00000     1.0    1.0  \n",
      "17  1.00000     1.0      1.00000     1.0    1.0  \n",
      "18  1.00000     1.0      1.00000     1.0    1.0  \n",
      "19  1.00000     1.0      1.00000     1.0    1.0  \n",
      "\n",
      "[20 rows x 36705 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Temp\\ipykernel_20912\\2697537730.py:1: RuntimeWarning: divide by zero encountered in log10\n",
      "  log_weighted_tf = 1 + np.where(X_tf.toarray() > 0, np.log10(X_tf.toarray()), 0)\n"
     ]
    }
   ],
   "source": [
    "log_weighted_tf = 1 + np.where(X_tf.toarray() > 0, np.log10(X_tf.toarray()), 0)\n",
    "log_weighted_df = pd.DataFrame(log_weighted_tf, columns=vectorizer_tf.get_feature_names_out())\n",
    "print(log_weighted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5ad77",
   "metadata": {},
   "source": [
    "## Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90c37421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Term  Document Frequency\n",
      "0               00                   1\n",
      "1              000                   3\n",
      "2         00021053                   1\n",
      "3         00081429                   1\n",
      "4         00482129                   1\n",
      "...            ...                 ...\n",
      "36700          zur                   1\n",
      "36701       zuriel                   1\n",
      "36702  zurishaddai                   1\n",
      "36703       zuyder                   1\n",
      "36704        zuzim                   1\n",
      "\n",
      "[36705 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_counts = (X_tf.toarray() > 0).sum(axis=0)\n",
    "df_table = pd.DataFrame({'Term': vectorizer_tf.get_feature_names_out(), 'Document Frequency': df_counts})\n",
    "print(df_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52c9b6",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bf403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Term  Document Frequency       IDF\n",
      "0               00                   1  1.301030\n",
      "1              000                   3  0.823909\n",
      "2         00021053                   1  1.301030\n",
      "3         00081429                   1  1.301030\n",
      "4         00482129                   1  1.301030\n",
      "...            ...                 ...       ...\n",
      "36700          zur                   1  1.301030\n",
      "36701       zuriel                   1  1.301030\n",
      "36702  zurishaddai                   1  1.301030\n",
      "36703       zuyder                   1  1.301030\n",
      "36704        zuzim                   1  1.301030\n",
      "\n",
      "[36705 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "N = len(combined_docs)\n",
    "idf_values = np.log10(N / df_counts)\n",
    "idf_table = pd.DataFrame({'Term': vectorizer_tf.get_feature_names_out(), 'Document Frequency': df_counts, 'IDF': idf_values})\n",
    "print(idf_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24b078",
   "metadata": {},
   "source": [
    "## TF-IDF Manual Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "771f705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         00       000  00021053  00081429  00482129       01        02  \\\n",
      "0   1.30103  1.071930   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "1   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "2   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "3   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "4   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "5   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "6   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "7   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "8   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "9   1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.692679   \n",
      "10  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "11  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "12  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "13  1.30103  1.895839   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "14  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "15  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "16  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "17  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "18  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "19  1.30103  0.823909   1.30103   1.30103   1.30103  1.30103  1.301030   \n",
      "\n",
      "          10       100  1000  ...  zorobabel       zso      zuar    zumpt  \\\n",
      "0   0.517732  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "1   0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "2   0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "3   1.721377  1.242874   1.0  ...   1.921779  1.301030  2.210411  1.30103   \n",
      "4   0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "5   0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "6   0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "7   0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "8   0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "9   0.397940  0.909381   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "10  0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "11  0.397940  0.698970   1.0  ...   1.301030  1.692679  1.301030  1.30103   \n",
      "12  0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "13  0.637524  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "14  0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "15  0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "16  0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "17  0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "18  0.734238  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "19  0.397940  0.698970   1.0  ...   1.301030  1.301030  1.301030  1.30103   \n",
      "\n",
      "        zuph       zur   zuriel  zurishaddai   zuyder    zuzim  \n",
      "0   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "1   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "2   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "3   1.921779  2.210411  1.30103     2.210411  1.30103  1.30103  \n",
      "4   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "5   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "6   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "7   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "8   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "9   1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "10  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "11  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "12  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "13  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "14  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "15  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "16  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "17  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "18  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "19  1.301030  1.301030  1.30103     1.301030  1.30103  1.30103  \n",
      "\n",
      "[20 rows x 36705 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf_manual = log_weighted_tf * idf_values\n",
    "tfidf_manual_df = pd.DataFrame(tfidf_manual, columns=vectorizer_tf.get_feature_names_out())\n",
    "print(tfidf_manual_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1884cb",
   "metadata": {},
   "source": [
    "## Term-Document Incidence Matrix: Talking Points\n",
    "\n",
    "- Using a Term-Document Incidence Matrix allows quick checks for whether specific keywords exist in a document, which is helpful in tasks like keyword-based filtering or search.\n",
    "\n",
    "- This matrix can be used to identify documents that contain all terms from a query phrase, although it does not provide information about the order or exact position of the terms.\n",
    "\n",
    "---\n",
    "\n",
    "## Term Frequency (TF): Talking Points\n",
    "\n",
    "- When two words appear equally often in a document, they receive the same TF score, even if one of them is a more generic or less meaningful term.\n",
    "\n",
    "- Normalizing term frequency ensures that document length does not unfairly influence which terms are considered most important when comparing different documents.\n",
    "\n",
    "---\n",
    "\n",
    "## Log Frequency Weight: Talking Points\n",
    "\n",
    "- Applying log frequency weighting reduces the influence of very frequently occurring words, preventing them from dominating the analysis in document search or ranking systems.\n",
    "\n",
    "- Adjusting raw counts with log frequency makes term importance values more stable and comparable, even when some words appear much more often than others.\n",
    "\n",
    "---\n",
    "\n",
    "## Document Frequency (DF): Talking Points\n",
    "\n",
    "- Document Frequency helps identify common filler or stopwords that appear in many documents, making it easier to filter out less informative terms automatically.\n",
    "\n",
    "- DF provides a broader view than just checking individual documents, as it measures how widely a term is distributed across the entire corpus.\n",
    "\n",
    "---\n",
    "\n",
    "## Inverse Document Frequency (IDF): Talking Points\n",
    "\n",
    "- High IDF values highlight terms that are rare and potentially more useful for distinguishing between documents, while low IDF values indicate common terms.\n",
    "\n",
    "- By reviewing IDF values, it becomes clear which terms will have a greater impact when calculating TF-IDF scores, helping to focus on more meaningful keywords.\n",
    "\n",
    "---\n",
    "\n",
    "## TF-IDF Weighting: Talking Points\n",
    "\n",
    "- TF-IDF emphasizes words that are both frequent within a document and rare across the entire corpus, making it a valuable method for document ranking and relevance scoring.\n",
    "\n",
    "- Comparing TF-IDF scores across documents reveals which terms contribute most to a document‚Äôs uniqueness and importance relative to a specific¬†query¬†or¬†task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d8b6b",
   "metadata": {},
   "source": [
    "## Step 6: The Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a255c3d",
   "metadata": {},
   "source": [
    "\n",
    "One team member must push the final notebook to GitHub and send the `.git` URL to the instructor before the end of class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c4ae5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß† Learning Objectives\n",
    "- Implement the foundations of **Vector Space Proximity** algorithms using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## üß© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* ‚Äì Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* ‚Äì NLP Pipeline and six IR basics techniques implementation + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* ‚Äì Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* ‚Äì Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - IR Basics & Vector Space Proximity Foundations Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## üíª Submission Checklist\n",
    "- ‚úÖ `IRBasics_VectorSpaceProximity.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, Inverted Index and the six concepts.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** (1-2 per concept)\n",
    "- ‚úÖ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ‚úÖ GitHub Repo:\n",
    "  - Public repo named `IRBasics-VectorSpaceProximity-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055fbe9",
   "metadata": {},
   "source": [
    "## üîö Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27358c8e",
   "metadata": {},
   "source": [
    "\n",
    "This workshop prepares you for our next session on **Vector Space Proximity** and **Cosine Similarity**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
